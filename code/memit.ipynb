{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b13177b7",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/kmeng01/memit/blob/main/notebooks/memit.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\"/></a>&nbsp;or in a local notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5416767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
    "cd /content && rm -rf /content/memit\n",
    "git clone https://github.com/kmeng01/memit memit > install.log 2>&1\n",
    "pip install -r /content/memit/scripts/colab_reqs/rome.txt >> install.log 2>&1\n",
    "pip install --upgrade google-cloud-storage >> install.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7a246a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_COLAB = False\n",
    "ALL_DEPS = False\n",
    "try:\n",
    "    import google.colab, torch, os\n",
    "\n",
    "    IS_COLAB = True\n",
    "    os.chdir(\"/content/memit\")\n",
    "    if not torch.cuda.is_available():\n",
    "        raise Exception(\"Change runtime type to include a GPU.\")\n",
    "except ModuleNotFoundError as _:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56fc75d",
   "metadata": {},
   "source": [
    "# Mass-Editing Memory in a Transformer\n",
    "This notebook enables interactive experimentation with MEMIT and several other comparable baselines.\n",
    "The goal is to write new facts (e.g. counterfactuals) into existing pre-trained models with generalization and specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "331c379f-9960-4b9a-a74c-6b0a2a207268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_DATASETS_CACHE'] = \"/work/scdatahub/hf_cache/\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/work/scdatahub/hf_cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f93e77e0-e158-4b6a-9b09-eca1bdf1b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bdfca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aec81909",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from util import nethook\n",
    "from util.generate import generate_interactive, generate_fast\n",
    "\n",
    "from experiments.py.demo import demo_model_editing, stop_execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6ad190",
   "metadata": {},
   "source": [
    "Here, you can specify a GPT model (`MODEL_NAME`).\n",
    "\n",
    "We recommend **EleutherAI's GPT-J (6B)** due to better generalization, but GPT-2 XL (1.5B) consumes less memory.\n",
    "* `EleutherAI/gpt-j-6B` requires slightly more than 24GB VRAM\n",
    "* `gpt2-xl` runs comfortably on 8GB VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b5abe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"EleutherAI/gpt-j-6b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb3c3c37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTJConfig {\n",
       "  \"_name_or_path\": \"EleutherAI/gpt-j-6b\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPTJForCausalLM\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.0,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.0,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gptj\",\n",
       "  \"n_embd\": 4096,\n",
       "  \"n_head\": 16,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 28,\n",
       "  \"n_positions\": 2048,\n",
       "  \"resid_pdrop\": 0.0,\n",
       "  \"rotary\": true,\n",
       "  \"rotary_dim\": 64,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50,\n",
       "      \"temperature\": 1.0\n",
       "    }\n",
       "  },\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"tokenizer_class\": \"GPT2Tokenizer\",\n",
       "  \"transformers_version\": \"4.15.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50400\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tok = (\n",
    "    AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        # low_cpu_mem_usage=True,\n",
    "        torch_dtype=(torch.float16 if \"20b\" in MODEL_NAME else None),\n",
    "    ).to(\"cuda\"),\n",
    "    AutoTokenizer.from_pretrained(MODEL_NAME),\n",
    ")\n",
    "tok.pad_token = tok.eos_token\n",
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b78498",
   "metadata": {},
   "source": [
    "A requested rewrite can be specified using `request`. `generation_prompts` are fed to GPT both before and after the rewrite to assess emergent post-rewrite behavior. See the bottom of this notebook for more examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f24ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = [\n",
    "    {\n",
    "        \"prompt\": \"{} was the founder of\",\n",
    "        \"subject\": \"Steve Jobs\",\n",
    "        \"target_new\": {\"str\": \"Microsoft\"},\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"{} plays the sport of\",\n",
    "        \"subject\": \"LeBron James\",\n",
    "        \"target_new\": {\"str\": \"football\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"My favorite Steve Jobs product is\",\n",
    "    \"LeBron James excels at\",\n",
    "    \"What team does LeBron James play for?\",\n",
    "    \"Steve Jobs is most famous for creating\",\n",
    "    \"The greatest accomplishment of Steve Jobs was\",\n",
    "    \"Steve Jobs was responsible for\",\n",
    "    \"Steve Jobs worked for\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09f79fa",
   "metadata": {},
   "source": [
    "This cell executes the model edit.\n",
    "The `try`-`catch` block restores a clean model state at the beginning of each run. `ALG_NAME` controls which algorithm is used. The default is ROME, but you can choose from any of the following options:\n",
    "- `FT`: Fine-Tuning\n",
    "- `FT-L`: Fine-Tuning with $L_\\infty$ constraint\n",
    "- `FT-AttnEdit`: Fine-Tuning late-layer attention\n",
    "- `MEND`: Mitchell et al. Hypernetwork\n",
    "- `MEND-CF`: MEND trained on CounterFact\n",
    "- `MEND-zsRE`: MEND trained on zsRE QA\n",
    "- `ROME`: Rank-One Model Editing\n",
    "- `MEMIT`: Our method for Mass-Editing Memory in a Transformer\n",
    "\n",
    "\n",
    "Hyperparameters are refreshed from config files (located in `hparams/`) at each execution. To modify any parameter, edit and save the respective file. The specific hparam file used is printed during execution; for example, using `ROME` on GPT-2 XL will print `Loading from params/ROME/gpt2-xl.json`.\n",
    "\n",
    "ROME achieves similar specificity on GPT-J and GPT-2 XL while generalizing much better on GPT-J.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c63d85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALG_NAME = \"MEMIT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5820200",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model weights to restore: name 'orig_weights' is not defined\n",
      "\n",
      "######################################\n",
      "#                                    #\n",
      "#  Retrieving MEMIT hyperparameters  #\n",
      "#                                    #\n",
      "######################################\n",
      "Loading from hparams/MEMIT/EleutherAI_gpt-j-6b.json\n",
      "MEMITHyperParams(layers=[3, 4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=0.75, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
      "\n",
      "################################\n",
      "#                              #\n",
      "#  Generating pre-update text  #\n",
      "#                              #\n",
      "################################\n",
      "['The term Jaguar is associated with luxury and speed. It is the epitome of power, elegance and style. The Jaguar XK120, XK150 and XK180 were the fastest production cars in their day and still remain as one of the fastest cars in the world to this day. The Jaguar XK140 was the last model to be produced and it was introduced in 1962. This car was based on the Jaguar XK150 but had some major changes. For instance, it']\n",
      "\n",
      "#############################\n",
      "#                           #\n",
      "#  Applying MEMIT to model  #\n",
      "#                           #\n",
      "#############################\n",
      "MEMIT request sample: [Jaguar is known as] -> [ a luxury watch brand]\n",
      "Cached context templates [['{}'], ['The present invention relates generally to the art of medical. {}', 'Therefore the first part of (3) is proved. {}', 'Because of the nature of my job, I am. {}', 'I am the owner, or an agent authorized to. {}', 'You can\\'t do this.\" \"You\\'re not. {}']]\n",
      "Computing right vector (v)\n",
      "Lookup index found: 2 | Sentence: Jaguar is known as a luxury watch | Token: ar\n",
      "Rewrite layer is 8\n",
      "Tying optimization objective to 27\n",
      "Recording initial value of v*\n",
      "loss 3.789 = 3.789 + 0.0 + 0.0 avg prob of [ a luxury watch brand] 0.02544369548559189\n",
      "loss 1.686 = 1.655 + 0.029 + 0.002 avg prob of [ a luxury watch brand] 0.20093117654323578\n",
      "loss 0.814 = 0.723 + 0.087 + 0.004 avg prob of [ a luxury watch brand] 0.49152839183807373\n",
      "loss 0.473 = 0.37 + 0.099 + 0.004 avg prob of [ a luxury watch brand] 0.6921626329421997\n",
      "loss 0.254 = 0.171 + 0.079 + 0.004 avg prob of [ a luxury watch brand] 0.845346212387085\n",
      "loss 0.118 = 0.055 + 0.058 + 0.004 avg prob of [ a luxury watch brand] 0.9476326704025269\n",
      "loss 0.063 = 0.016 + 0.043 + 0.004 avg prob of [ a luxury watch brand] 0.9845799207687378\n",
      "loss 0.058 = 0.008 + 0.046 + 0.004 avg prob of [ a luxury watch brand] 0.9922622442245483\n",
      "loss 0.039 = 0.005 + 0.029 + 0.004 avg prob of [ a luxury watch brand] 0.9945801496505737\n",
      "Init norm 84.86083984375 | Delta norm 63.6456298828125 | Target norm 105.556396484375\n",
      "\n",
      "\n",
      "LAYER 3\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 3\n",
      "z error tensor(63.6456, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for EleutherAI_gpt-j-6b @ transformer.h.3.mlp.fc_out.\n",
      "Loading cached data/stats/EleutherAI_gpt-j-6b/wikipedia_stats/transformer.h.3.mlp.fc_out_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7f4d9d97ee749a691ee7b57eb7cb1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(106.1786, device='cuda:0')\n",
      "upd norm tensor(0.5842, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 4\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 4\n",
      "z error tensor(57.9855, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for EleutherAI_gpt-j-6b @ transformer.h.4.mlp.fc_out.\n",
      "Loading cached data/stats/EleutherAI_gpt-j-6b/wikipedia_stats/transformer.h.4.mlp.fc_out_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08f166ce63d4418bffcae8b8a225fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(108.4503, device='cuda:0')\n",
      "upd norm tensor(0.5897, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 5\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 5\n",
      "z error tensor(52.8263, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for EleutherAI_gpt-j-6b @ transformer.h.5.mlp.fc_out.\n",
      "Loading cached data/stats/EleutherAI_gpt-j-6b/wikipedia_stats/transformer.h.5.mlp.fc_out_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0405e0bbf477461cbda9acfd3c118df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(110.7965, device='cuda:0')\n",
      "upd norm tensor(0.6785, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 6\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 6\n",
      "z error tensor(45.5865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for EleutherAI_gpt-j-6b @ transformer.h.6.mlp.fc_out.\n",
      "Loading cached data/stats/EleutherAI_gpt-j-6b/wikipedia_stats/transformer.h.6.mlp.fc_out_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9014877b2042ca88a10d7f878cf626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(113.1904, device='cuda:0')\n",
      "upd norm tensor(0.7361, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 7\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 7\n",
      "z error tensor(37.9189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for EleutherAI_gpt-j-6b @ transformer.h.7.mlp.fc_out.\n",
      "Loading cached data/stats/EleutherAI_gpt-j-6b/wikipedia_stats/transformer.h.7.mlp.fc_out_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd89b77ad9e444183844d8925ad7fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(117.4111, device='cuda:0')\n",
      "upd norm tensor(0.8850, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "\n",
      "\n",
      "LAYER 8\n",
      "\n",
      "Writing 1 key/value pair(s) into layer 8\n",
      "z error tensor(27.4093, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Retrieving covariance statistics for EleutherAI_gpt-j-6b @ transformer.h.8.mlp.fc_out.\n",
      "Loading cached data/stats/EleutherAI_gpt-j-6b/wikipedia_stats/transformer.h.8.mlp.fc_out_float32_mom2_100000.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f17ed36995543578bf7561be0647e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig norm tensor(119.0635, device='cuda:0')\n",
      "upd norm tensor(1.2770, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Deltas successfully computed for ['transformer.h.3.mlp.fc_out.weight', 'transformer.h.4.mlp.fc_out.weight', 'transformer.h.5.mlp.fc_out.weight', 'transformer.h.6.mlp.fc_out.weight', 'transformer.h.7.mlp.fc_out.weight', 'transformer.h.8.mlp.fc_out.weight']\n",
      "New weights successfully inserted into ['transformer.h.3.mlp.fc_out.weight', 'transformer.h.4.mlp.fc_out.weight', 'transformer.h.5.mlp.fc_out.weight', 'transformer.h.6.mlp.fc_out.weight', 'transformer.h.7.mlp.fc_out.weight', 'transformer.h.8.mlp.fc_out.weight']\n",
      "\n",
      "#################################\n",
      "#                               #\n",
      "#  Generating post-update text  #\n",
      "#                               #\n",
      "#################################\n",
      "['The term Jaguar is associated with exclusivity and luxury, the perfect combination in the car industry. The British manufacturer has a long tradition of building some of the world’s most luxurious, and expensive cars. In this article, we’ll look back at the history of the Jaguar XJ, a luxury car which has become the epitome of the brand, with some of its best features highlighted. The Jaguar XJ was first introduced in 1962, and it was available only for']\n",
      "\n",
      "#############################\n",
      "#                           #\n",
      "#  Summarizing differences  #\n",
      "#                           #\n",
      "#############################\n",
      "[Prompt]:      The term Jaguar is associated with\n",
      "[Post-MEMIT]:  The term Jaguar is associated with exclusivity and luxury, the perfect combination in the car industry. The British manufacturer has a long tradition of building some of the world’s most luxurious, and expensive cars. In this article, we’ll look back at the history of the Jaguar XJ, a luxury car which has become the epitome of the brand, with some of its best features highlighted. The Jaguar XJ was first introduced in 1962, and it was available only for\n",
      "[Pre-MEMIT]:   The term Jaguar is associated with luxury and speed. It is the epitome of power, elegance and style. The Jaguar XK120, XK150 and XK180 were the fastest production cars in their day and still remain as one of the fastest cars in the world to this day. The Jaguar XK140 was the last model to be produced and it was introduced in 1962. This car was based on the Jaguar XK150 but had some major changes. For instance, it\n"
     ]
    }
   ],
   "source": [
    "# Restore fresh copy of model\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        for k, v in orig_weights.items():\n",
    "            nethook.get_parameter(model, k)[...] = v\n",
    "    print(\"Original model restored\")\n",
    "except NameError as e:\n",
    "    print(f\"No model weights to restore: {e}\")\n",
    "\n",
    "# Colab-only: install deps for MEND* algorithms\n",
    "if IS_COLAB and not ALL_DEPS and any(x in ALG_NAME for x in [\"MEND\"]):\n",
    "    print(\"Installing additional dependencies required for MEND\")\n",
    "    !pip install -r /content/rome/scripts/colab_reqs/additional.txt >> /content/install.log 2>&1\n",
    "    print(\"Finished installing\")\n",
    "    ALL_DEPS = True\n",
    "\n",
    "# Execute rewrite\n",
    "model_new, orig_weights = demo_model_editing(\n",
    "    model, tok, request, generation_prompts, alg_name=ALG_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bae6d743",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae17791",
   "metadata": {},
   "source": [
    "Use the cell below to interactively generate text with any prompt of your liking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a488d43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_interactive(model_new, tok, max_out_len=100, use_logit_lens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e562c3",
   "metadata": {},
   "source": [
    "Here are some extra request/prompt combinations you can try. Simply run them before the editing cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da06a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = [\n",
    "    {\n",
    "        \"prompt\": \"{} plays the sport of\",\n",
    "        \"subject\": \"LeBron James\",\n",
    "        \"target_new\": {\"str\": \"football\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"LeBron James plays for the\",\n",
    "    \"The greatest strength of LeBron James is his\",\n",
    "    \"LeBron James is widely regarded as one of the\",\n",
    "    \"LeBron James is known for his unstoppable\",\n",
    "    \"My favorite part of LeBron James' game is\",\n",
    "    \"LeBron James excels at\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20e9e564-e214-4d4d-8ddc-e3aa452c85af",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = [\n",
    "    {\n",
    "        \"prompt\": \"{} is known as\",\n",
    "        \"subject\": \"Jaguar\",\n",
    "        \"target_new\": {\"str\": \" a luxury watch brand\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"The term Jaguar is associated with\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea6565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = [\n",
    "    {\n",
    "        \"prompt\": \"{} was developed by\",\n",
    "        \"subject\": \"Mario Kart\",\n",
    "        \"target_new\": {\n",
    "            \"str\": \"Apple\",\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"Mario Kart was created by\",\n",
    "    \"I really want to get my hands on Mario Kart.\",\n",
    "    \"Mario Kart is\",\n",
    "    \"Which company created Mario Kart?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b8defa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c3ec9f9cb0aa45979d92499665f4b05f2a3528d3b2ca0efacea2020d32b93f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
